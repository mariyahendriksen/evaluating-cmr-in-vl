# Evaluating Image-Text Retrieval in Multimodal Models

This repository contains the code for the internship project of 'Evaluating Image-Text Retrieval in Multimodal Models'.

## Project Organization
------------

    â”œâ”€â”€ Makefile           <- Makefile with commands like `make data` or `make test`
    â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
    |
    â”œâ”€â”€ src                <- Source code for use in this project.
    â”‚Â Â  â”œâ”€â”€ __init__.py    <- Makes directory a Python module
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ main.py        <- Entry-point for the CLI
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ data           <- Package to download or generate data
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ features       <- Package to turn raw data into features for modeling
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ models         <- Package to train models and then use trained models to make
    â”‚   â”‚                     predictions
    â”‚Â Â  â””â”€â”€ visualization  <- Package to create exploratory and results oriented visualizations
    â”‚
    â”œâ”€â”€ data
    â”‚Â Â  â”œâ”€â”€ external       <- Data from third party sources.
    â”‚Â Â  â”œâ”€â”€ features       <- Processed data that has been converted to features.
    â”‚Â Â  â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
    â”‚Â Â  â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
    â”‚Â Â  â””â”€â”€ raw            <- The original, immutable data dump.
    â”‚
    â”œâ”€â”€ docs               <- A default Sphinx project; see sphinx-doc.org for details
    â”‚
    â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
    â”‚
    â”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    â”‚                         the creator's initials, and a short `-` delimited description, e.g.
    â”‚                         `1.0-jqp-initial-data-exploration`.
    â”‚
    â”œâ”€â”€ jobs               <- Data science platform job specs. This folder contains all the DSP YAML job specs.
    â”‚                         You can use these to store the launch specs for various kinds of jobs including the parameters.
    â”‚                         For any spec present in this folder (for eg. example-job.yaml),
    â”‚                         you can launch a DSP job with that spec using `make launch-${spec-name}`
    â”‚                         (for eg. `make launch-example-job`).
    â”‚
    â”œâ”€â”€ references         <- Data dictionaries, manuals, and all other explanatory materials.
    â”‚
    â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    â”‚Â Â  â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
    â”‚
    â”œâ”€â”€ tests
    â”‚Â Â  â”œâ”€â”€ unit           <- Unit tests.
    â”‚Â Â  â””â”€â”€ integration    <- Integration tests.
    |
    â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    â”‚                         generated with `pip freeze > requirements.txt`
    â”‚
    â”œâ”€â”€ setup.py           <- Makes project pip installable (pip install -e .) so it can be imported.
    â””â”€â”€ setup.cfg          <- Config file with settings for flake8 and isort


## Setting Dev Environment

1. Create a clean virtual environment and install the requirements for this project
```
make create-venv
```
2. Activate virtual environment via -
```
source .venv/bin/activate
```

## Make targets


1. Run `make test` to verify everything is in order.
2. Run `make lint` to replicate the Jenkins linting tests locally.
3. Run `make fix-lint` to automatically fix some of these errors.
4. Run `make data` to generate data.
5. Run `make features` to generate features.
6. Run `make model` to generate models.
7. Run `make figures` to generate figures.
8. Run `make setup-dsp` to setup Katie, the CLI for the [Data Science Platform.](https://bbgithub.dev.bloomberg.com/pages/ds/ds-platform-ctrl/)
9. Run `make launch-dsp-notebook` to launch a Jupyter notebook with this repository on the Data Science Platform. To change the Jupyter job config, you can modify `notebook.yaml`. To learn more about how to use YAMLs with the Data Science Platform, [remoteio](https://bbgithub.dev.bloomberg.com/nlu/remoteio) and other DSP tips and tricks check out the [DSP Tips talk](http://bburl/guild-week-dsp-tips)

You can modify the targets to generate features, data etc. in the `Makefile`

## Features

This template is based on [the datascience cookiecutter](http://drivendata.github.io/cookiecutter-data-science/) but
it has been adapted for internal Bloomberg use and comes baked with features related to the [Data Science Platform.](https://bbgithub.dev.bloomberg.com/pages/ds/ds-platform-ctrl/)

----------

<small> ðŸ”† This project was generated by [cc-research-project v1.1.8](https://bbgithub.dev.bloomberg.com/ml-guild/cc-research-project/releases/tag/v1.1.8) ðŸ”† </small>

<!-- Project generated by cc-research-project v1.1.8. Please do not remove this comment or the above line. -->

